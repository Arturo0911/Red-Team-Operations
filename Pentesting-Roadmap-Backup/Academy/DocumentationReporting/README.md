# Documentation and Reporting HTB Academy.


## Notetaking sample structure:

* **Attack path**: screenshots in (external or internal pentest) with
                commands output will make it easier to report.

* **Credentials**: A centralized place to keep your compromised credentials 
                and secrets as you go along.

* **Findings**: Create a subfolder for each finding and then  writing our 
                narrative and saving in the folder along with any evidence 
                (screenshots, commands).

* **Vulnerability Scan Research**: Section to take notes on things
you've researched and tried with your vulnerability scans
(so you don't end up redoing work you already did).

* **Service Enumeration Resesarch**: A section to take notes on 
which services you've investigated, failed exploitation attempts,
promising vulnerabilities/missconifgurations, etc.

* **Web application research**: A section to note down interesting
web applications found through various methods, (fuzer subdomains). 
Credentials you've tried and worked!

* **AD Enumeration Research**: A section for showing, step-by-step,
waht Active Directory enumeration you've already performed. Note
down any areas of intesrest you need to run down later in the 
assessment.

* **OSINT**: A section to keep track of interesting information 
you've collected via OSINT, if applicable to the engagement.

* **Administrative information**: Some people may find it helpful
to have a centralized location to store contact information for
other project stakeholders like Project Managers (PMs) or client
points of contact (POCs), unique objectives/flags defined in the
Rules of Engangement (RoE), and other ithems that you find
yourself often referencing throught the project. It can also
be used as running to-do list. As ideas pop up for testing that
you need to perform or want to try but don't have time for, be 
diligent about writing them down here so you can come back to them
later.

* **Scoping Information**: Here we can store information about
in-scope IP/addresses/CIDR ranges, web applications URLs, and any
credentials for web applications, VPN, or AD provided by the client.
It could also include anything else pertinent to the scope of 
the assessment so we don't have to keep re opening scope 
information and ensure that we don't stray from the scope of
the assessment.

* **Activitvy Log**: High-level tracking of everything you did 
during the asessment for possible event correlation.

* **Payload Log**: Similar to the activity log, tracking the payloads
you're using (and file hash for anything uploaded and the upload location)
 in a client environment is critical. 


## Evidence

No matter the asessment type, our client (typically) does not care
about the cool exploit chains we pull off or how easily we "pwned"
their network. Ultimately, they are paying for the report deliverable,
which should clearly communicate the issues discovered and 
evidence that can be used for validation and reproduction.
Without clear evidence, it can be challenging for internal 
security teams, sysadmins, devs, etc., to reproduce our work while
working to implement a fix or even to understand the nature of 
the issue.



## Wath to capture

<p>
As we know, each finding will need to have evidence. It may also
be prudent to collect evidence of the tests that were performed that were 
unsuccessful in case the client questions your thoroughness. If
you're working on the command line, Tmux logs may be sufficient
evidence to paste into the report as literal terminal output, but 
they can be horribly formatted. For this reason, capturing your 
terminal output for significant steps as you go along and tracking
that seaprately alongside your findings, is a good idea. For everything
else, screenshots should be taken.
</p>


## Components of a Report.

As mentioned previously, the report is the main deliverable that a client is paying for
when they contract your firm to perform a penetration test. The report is our chance to
show off our work during the assessment and provide the client with as much value as
possible. Ideally, the report will be free of extraneous data and information that "clutter"
up the report or distract from teh issues we are trying to convey of the overall picture of
their security posture we are trying to paint. Everything in the report should have a 
reason for being therem and we don't want to overwhelm the reader (for example, don't 
paste in 50+ pages of console output!). In this section, we'll cover the key elements of a
report and how we can best structure if to show off our work and help our clients
prioritieze remediation.


## Prioritizing Our Efforts

During an assessment, especially large ones, we'll be faced with a lot of "noise" that we
need to filter out to best focus our efforst and prioritize findings. As testers, we are
required to disclose everything we find, but when there is a ton of information coming
at us through scans and enumeration, it is easy to get lost or fogus on the wrong thins
and wast etime and potentially miss high-impact issues. This is whhy it is essential that
we understand the output that our tools produce, have repeteable steps (such as 
scripts or other tools) to sift through all of this data, process it, and remove false 
positivies or informational issues that could diustract us from the goal of the 
assessment. Experience and a repetable process are key so that we can sift through
all of our data and focus our efforst on high-impact findings such as remote code
execution (RCE) flaws or others that may lead to sensitive data disclosure. It is worth it
(and our duty) to report informational findings, but instead of spending the majority of
our t5ime validating these minor, non-exploitable issues, you may want to consider
consolidating some of them into categories taht show the client you were aware that 
the issues existed, but you were unable to exploit them in any meaningful way (e.h., 35
differetn variations of problems with SSL/TLS, a ton of DOS vulnerabilitiers in an EOL 
version of PHP, etc.).

When starting in penetration testing, it can be difficult to know what to prioritize, and
we may fall down rabbit holes trying to exploit a flaw that doesn't exist or getting a 
broken PoC exploit to work. Time and experience help here, but we should also learn on 
senior team members and mentors to help something that you may waste half a day 
on could be something that they have seen many times and could tell you quickly
whether it is a false positivie or wordh running down. Even if they can't give you really
quick black and white answer, they can at least point you in a direction that saves you
several hours. Surround yourseld with people you're comfortable with asking for help
that won't make you feel like an idiot if you don't know all the answers.



## Writing an Attack Chain

The attack chain is our chance to show off the cool exploitation chain we took to gain a 
foothold, move laterally, and compromise the domain, It can be a helpful mechanism to
help the reader connect the dots when multiple findings are used in conjunction with
each other and gain a better understading of why certain findings are given the
severity rating they are assigned. For example, a particula finding on its own may
be medium-risk but, combined with one or two other issues, could elevate it to high-
risk, and this section is our chance to demostrate that. A common example is using 
Responder to intercept NBT-NS/LLMNR traffic and relaying it to hosts where SMB
signing is not represent. It can get really interesting if some findings can be incorporated
taht might otherwise seem inconsequeantial, like using and information disclosure of
some sort to help guide you through an LFI to read an interesting configuration file, log
in to an external-facing application, and leverate functionality to gain remote code
execution and a foothold inside the internal network.

